#!/usr/bin/env python3
"""
Final comprehensive test before git push

Author: Hyunsik Kim <avantkim@gmail.com>
Date: May 2025
"""

import sys
import os
import time
import json
from pathlib import Path

# Add the tokenizers package to the path
sys.path.insert(0, 'bindings/python')

from tokenizers.implementations import ByteLevelBPETokenizer, UTF16ByteLevelBPETokenizer
from tokenizers import pre_tokenizers

def verify_implementation():
    """Verify that the implementation is working correctly"""
    print("="*80)
    print("STEP 1: IMPLEMENTATION VERIFICATION")
    print("="*80)
    
    try:
        # Test imports
        print("âœ“ Testing imports...")
        from tokenizers.implementations import UTF16ByteLevelBPETokenizer
        from tokenizers import pre_tokenizers, decoders, processors
        
        # Test alphabet
        print("âœ“ Testing alphabets...")
        bl_alphabet = pre_tokenizers.ByteLevel.alphabet()
        utf16_alphabet = pre_tokenizers.UTF16ByteLevel.alphabet()
        
        assert len(bl_alphabet) == 256, f"ByteLevel alphabet should have 256 chars, got {len(bl_alphabet)}"
        assert len(utf16_alphabet) == 256, f"UTF16ByteLevel alphabet should have 256 chars, got {len(utf16_alphabet)}"
        
        print(f"  - ByteLevel alphabet: {len(bl_alphabet)} characters")
        print(f"  - UTF16ByteLevel alphabet: {len(utf16_alphabet)} characters")
        
        # Test tokenizer creation
        print("âœ“ Testing tokenizer creation...")
        bl_tokenizer = ByteLevelBPETokenizer()
        utf16_tokenizer = UTF16ByteLevelBPETokenizer()
        
        print("âœ“ All implementation tests passed!")
        return True
        
    except Exception as e:
        print(f"âœ— Implementation test failed: {e}")
        return False

def train_tokenizers():
    """Train both tokenizers on the same dataset"""
    print("\n" + "="*80)
    print("STEP 2: TOKENIZER TRAINING")
    print("="*80)
    
    # Check training data
    training_file = Path("~/work/data/utf16_tokenizer_evaluation/training_data/multilingual_training_10mb.txt").expanduser()
    if not training_file.exists():
        print(f"âœ— Training file not found: {training_file}")
        return None
    
    print(f"âœ“ Training file found: {training_file}")
    print(f"  Size: {training_file.stat().st_size / (1024*1024):.2f} MB")
    
    # Create output directories
    results_dir = Path("~/work/data/utf16_tokenizer_evaluation/results").expanduser()
    bl_vocab_dir = results_dir / "byte_level_vocab"
    utf16_vocab_dir = results_dir / "utf16_byte_level_vocab"
    
    bl_vocab_dir.mkdir(exist_ok=True)
    utf16_vocab_dir.mkdir(exist_ok=True)
    
    vocab_size = 1000
    
    try:
        # Train ByteLevel BPE
        print(f"\nğŸ”„ Training ByteLevelBPETokenizer (vocab_size={vocab_size})...")
        start_time = time.time()
        
        bl_tokenizer = ByteLevelBPETokenizer()
        bl_tokenizer.train([str(training_file)], vocab_size=vocab_size, min_frequency=1)
        bl_tokenizer.save_model(str(bl_vocab_dir))
        
        bl_train_time = time.time() - start_time
        print(f"âœ“ ByteLevel training completed in {bl_train_time:.2f} seconds")
        
        # Train UTF16ByteLevel BPE
        print(f"\nğŸ”„ Training UTF16ByteLevelBPETokenizer (vocab_size={vocab_size})...")
        start_time = time.time()
        
        utf16_tokenizer = UTF16ByteLevelBPETokenizer()
        utf16_tokenizer.train([str(training_file)], vocab_size=vocab_size, min_frequency=1)
        utf16_tokenizer.save_model(str(utf16_vocab_dir))
        
        utf16_train_time = time.time() - start_time
        print(f"âœ“ UTF16ByteLevel training completed in {utf16_train_time:.2f} seconds")
        
        return {
            'bl_tokenizer': bl_tokenizer,
            'utf16_tokenizer': utf16_tokenizer,
            'bl_train_time': bl_train_time,
            'utf16_train_time': utf16_train_time
        }
        
    except Exception as e:
        print(f"âœ— Training failed: {e}")
        return None

def performance_comparison(tokenizers_info):
    """Perform comprehensive performance comparison"""
    print("\n" + "="*80)
    print("STEP 3: PERFORMANCE COMPARISON")
    print("="*80)
    
    bl_tokenizer = tokenizers_info['bl_tokenizer']
    utf16_tokenizer = tokenizers_info['utf16_tokenizer']
    
    # Test cases
    test_cases = [
        ("Korean", [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬",
            "í•œêµ­ì–´ í† í°í™” í…ŒìŠ¤íŠ¸",
            "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬",
            "ëŒ€í•œë¯¼êµ­ í™”ì´íŒ…"
        ]),
        ("Chinese", [
            "ä½ å¥½ä¸–ç•Œ",
            "ä¸­æ–‡åˆ†è¯æµ‹è¯•",
            "åŒ—äº¬å¤§å­¦",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯",
            "è‡ªç„¶è¯­è¨€å¤„ç†"
        ]),
        ("English", [
            "Hello World",
            "Natural Language Processing",
            "Machine Learning",
            "Artificial Intelligence",
            "Deep Learning"
        ]),
        ("Mixed", [
            "ì•ˆë…•í•˜ì„¸ìš” Hello ä½ å¥½",
            "Korean í•œêµ­ì–´ Chinese ä¸­æ–‡",
            "AI ì¸ê³µì§€ëŠ¥ äººå·¥æ™ºèƒ½",
            "Seoul ì„œìš¸ åŒ—äº¬ Beijing",
            "Technology ê¸°ìˆ  æŠ€æœ¯"
        ])
    ]
    
    results = {}
    
    for language, texts in test_cases:
        print(f"\nğŸ“Š Testing {language} language:")
        print("-" * 50)
        
        bl_tokens_total = 0
        utf16_tokens_total = 0
        bl_errors = 0
        utf16_errors = 0
        
        for text in texts:
            try:
                # ByteLevel tokenization
                bl_encoded = bl_tokenizer.encode(text)
                bl_decoded = bl_tokenizer.decode(bl_encoded.ids)
                bl_tokens = len(bl_encoded.tokens)
                bl_tokens_total += bl_tokens
                
                if bl_decoded != text:
                    bl_errors += 1
                    print(f"  âš ï¸  ByteLevel roundtrip error: '{text}' -> '{bl_decoded}'")
                
            except Exception as e:
                bl_errors += 1
                bl_tokens_total += 999  # Penalty for errors
                print(f"  âœ— ByteLevel error on '{text}': {e}")
            
            try:
                # UTF16ByteLevel tokenization
                utf16_encoded = utf16_tokenizer.encode(text)
                utf16_decoded = utf16_tokenizer.decode(utf16_encoded.ids)
                utf16_tokens = len(utf16_encoded.tokens)
                utf16_tokens_total += utf16_tokens
                
                if utf16_decoded != text:
                    utf16_errors += 1
                    print(f"  âš ï¸  UTF16ByteLevel roundtrip error: '{text}' -> '{utf16_decoded}'")
                
            except Exception as e:
                utf16_errors += 1
                utf16_tokens_total += 999  # Penalty for errors
                print(f"  âœ— UTF16ByteLevel error on '{text}': {e}")
        
        # Calculate averages
        bl_avg = bl_tokens_total / len(texts)
        utf16_avg = utf16_tokens_total / len(texts)
        
        # Calculate improvement
        if bl_avg > 0:
            improvement = (bl_avg - utf16_avg) / bl_avg * 100
        else:
            improvement = 0
        
        results[language] = {
            'bl_avg_tokens': bl_avg,
            'utf16_avg_tokens': utf16_avg,
            'bl_errors': bl_errors,
            'utf16_errors': utf16_errors,
            'improvement_percent': improvement,
            'test_count': len(texts)
        }
        
        print(f"  ByteLevel BPE: {bl_avg:.1f} avg tokens, {bl_errors} errors")
        print(f"  UTF16ByteLevel BPE: {utf16_avg:.1f} avg tokens, {utf16_errors} errors")
        
        if improvement > 0:
            print(f"  ğŸ‰ UTF16 improvement: {improvement:.1f}% fewer tokens")
        elif improvement < 0:
            print(f"  ğŸ“ˆ ByteLevel advantage: {-improvement:.1f}% fewer tokens")
        else:
            print(f"  âš–ï¸  Equal performance")
    
    return results

def main():
    """Main test function"""
    print("ğŸš€ FINAL TEST BEFORE GIT PUSH")
    print("UTF16ByteLevelBPETokenizer Implementation")
    print("Author: Hyunsik Kim <avantkim@gmail.com>")
    print("Date: May 2025")
    
    # Step 1: Verify implementation
    if not verify_implementation():
        print("\nâŒ Implementation verification failed!")
        return False
    
    # Step 2: Train tokenizers
    tokenizers_info = train_tokenizers()
    if not tokenizers_info:
        print("\nâŒ Tokenizer training failed!")
        return False
    
    # Step 3: Performance comparison
    performance_results = performance_comparison(tokenizers_info)
    
    print("\n" + "="*80)
    print("ğŸ‰ FINAL TEST COMPLETED SUCCESSFULLY!")
    print("="*80)
    print("âœ… Implementation verified")
    print("âœ… Tokenizers trained successfully")
    print("âœ… Performance comparison completed")
    print("\nğŸš€ Ready for git push!")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 